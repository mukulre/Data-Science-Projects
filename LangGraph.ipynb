{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80baa661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.5)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
      "Requirement already satisfied: langchain-ollama in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
      "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.2.6)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.1)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.12.3)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from langchain-ollama) (0.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.6.1)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.13.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph) (1.12.1)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.32.4)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n"
     ]
    }
   ],
   "source": [
    "pip install langgraph langchain langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f92ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "# The State dictates what data flows through the graph.\n",
    "# Here, we just track the list of messages in the conversation.\n",
    "class AgentState(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    category: str # We will store the classification here (Math vs General)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2f08b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Ollama backend for LLM\n"
     ]
    }
   ],
   "source": [
    "# LLM setup: prefer Ollama, fall back to OpenAI if unavailable\n",
    "import os\n",
    "from typing import Optional\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "llm = None\n",
    "llm_provider: Optional[str] = None\n",
    "\n",
    "# Try Ollama first\n",
    "try:\n",
    "    from langchain_ollama import ChatOllama\n",
    "    # Note: the `ollama` CLI must be installed and running for remote calls to succeed\n",
    "    llm = ChatOllama(model=\"llama3\", temperature=0)\n",
    "    llm_provider = \"ollama\"\n",
    "    print('Using Ollama backend for LLM')\n",
    "except Exception as e:\n",
    "    print('Ollama unavailable:', e)\n",
    "    # Try OpenAI via LangChain as a fallback (requires OPENAI_API_KEY in env)\n",
    "    try:\n",
    "        from langchain.chat_models import ChatOpenAI\n",
    "        if os.environ.get('OPENAI_API_KEY'):\n",
    "            llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "            llm_provider = \"openai\"\n",
    "            print('Using OpenAI fallback for LLM')\n",
    "        else:\n",
    "            print('OPENAI_API_KEY not set; OpenAI fallback disabled')\n",
    "    except Exception as e2:\n",
    "        print('OpenAI LangChain client unavailable:', e2)\n",
    "\n",
    "def send_prompt(prompt: str) -> str:\n",
    "    \"\"\"Send a prompt to the available LLM and return plain text.\"\"\"\n",
    "    if llm is None:\n",
    "        raise RuntimeError('No LLM available. Install Ollama or set OPENAI_API_KEY for OpenAI fallback.')\n",
    "    # Ollama client exposes `invoke` which returns a message-like object with `content`\n",
    "    if llm_provider == 'ollama':\n",
    "        resp = llm.invoke(prompt)\n",
    "        return getattr(resp, 'content', str(resp)).strip()\n",
    "    # For LangChain ChatOpenAI try common call patterns\n",
    "    try:\n",
    "        # Some LangChain LLMs implement `predict` which returns a string\n",
    "        if hasattr(llm, 'predict'):\n",
    "            return llm.predict(prompt).strip()\n",
    "        # Fallback to calling the object directly\n",
    "        out = llm(prompt)\n",
    "        # If it returns an LLMResult-like object, try to extract text\n",
    "        if hasattr(out, 'generations'):\n",
    "            gens = out.generations\n",
    "            if gens and gens[0] and hasattr(gens[0][0], 'text'):\n",
    "                return gens[0][0].text.strip()\n",
    "        return str(out).strip()\n",
    "    except Exception as e:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b4754ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 1: The Categorizer\n",
    "def categorize_input(state: AgentState):\n",
    "    \"\"\"\n",
    "    Analyzes the user's last message and decides if it's 'Math' or 'General'.\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1].content\n",
    "    prompt = f\"You are a router. Classify the following input as either 'Math' or 'General'. Return ONLY the word 'Math' or 'General'. Do not add punctuation.\\n\\nInput: {last_message}\"\n",
    "    try:\n",
    "        category_text = send_prompt(prompt)\n",
    "        category = category_text.strip()\n",
    "    except Exception as e:\n",
    "        print('Categorization failed:', e)\n",
    "        # Default to General if classification fails\n",
    "        category = 'General'\n",
    "    return {\"category\": category}\n",
    "\n",
    "# Node 2: The Math Expert\n",
    "def handle_math(state: AgentState):\n",
    "    print(\"--- ðŸ§® Entering Math Node ---\")\n",
    "    last_message = state[\"messages\"][-1].content\n",
    "    try:\n",
    "        text = send_prompt(f\"You are a mathematician. Solve this simply: {last_message}\")\n",
    "    except Exception as e:\n",
    "        print('Math node LLM error:', e)\n",
    "        text = f\"Error: {e}\"\n",
    "    return {\"messages\": [AIMessage(content=text)]}\n",
    "\n",
    "# Node 3: The General Chat\n",
    "def handle_general(state: AgentState):\n",
    "    print(\"--- ðŸ’¬ Entering General Chat Node ---\")\n",
    "    last_message = state[\"messages\"][-1].content\n",
    "    try:\n",
    "        text = send_prompt(f\"You are a helpful assistant. Reply to: {last_message}\")\n",
    "    except Exception as e:\n",
    "        print('General node LLM error:', e)\n",
    "        text = f\"Error: {e}\"\n",
    "    return {\"messages\": [AIMessage(content=text)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5aa5424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def routing_logic(state: AgentState):\n",
    "    # If the category is Math, go to 'math_node'\n",
    "    if state[\"category\"] == \"Math\":\n",
    "        return \"math_node\"\n",
    "    # Otherwise, go to 'general_node'\n",
    "    return \"general_node\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29974dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# 1. Initialize the Graph with our State structure\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# 2. Add the Nodes\n",
    "workflow.add_node(\"categorizer\", categorize_input)\n",
    "workflow.add_node(\"math_node\", handle_math)\n",
    "workflow.add_node(\"general_node\", handle_general)\n",
    "\n",
    "# 3. Set the Entry Point\n",
    "# When the graph starts, the first person to touch the ball is the 'categorizer'\n",
    "workflow.set_entry_point(\"categorizer\")\n",
    "\n",
    "# 4. Add Conditional Edges\n",
    "# After 'categorizer' runs, look at 'routing_logic' to decide where to go next.\n",
    "workflow.add_conditional_edges(\n",
    "    \"categorizer\",        # From this node...\n",
    "    routing_logic,        # ...run this logic...\n",
    "    {                     # ...and map the output to a node.\n",
    "        \"math_node\": \"math_node\",\n",
    "        \"general_node\": \"general_node\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# 5. Add Normal Edges\n",
    "# After math or general chat, we are done. Go to END.\n",
    "workflow.add_edge(\"math_node\", END)\n",
    "workflow.add_edge(\"general_node\", END)\n",
    "\n",
    "# 6. Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "821f864e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TEST 1: Math ---\n",
      "Categorization failed: [Errno 111] Connection refused\n",
      "Finished running: categorizer\n",
      "--- ðŸ’¬ Entering General Chat Node ---\n",
      "General node LLM error: [Errno 111] Connection refused\n",
      "Finished running: general_node\n",
      "\n",
      "--- TEST 2: General ---\n",
      "Categorization failed: [Errno 111] Connection refused\n",
      "Finished running: categorizer\n",
      "--- ðŸ’¬ Entering General Chat Node ---\n",
      "General node LLM error: [Errno 111] Connection refused\n",
      "Finished running: general_node\n"
     ]
    }
   ],
   "source": [
    "# Test 1: A Math Question\n",
    "print(\"\\n--- TEST 1: Math ---\")\n",
    "inputs_1 = {\"messages\": [HumanMessage(content=\"What is 55 multiplied by 10?\")]}\n",
    "\n",
    "# Stream the output to see the steps\n",
    "for event in app.stream(inputs_1):\n",
    "    for key, value in event.items():\n",
    "        print(f\"Finished running: {key}\")\n",
    "\n",
    "# Test 2: A Casual Greeting\n",
    "print(\"\\n--- TEST 2: General ---\")\n",
    "inputs_2 = {\"messages\": [HumanMessage(content=\"Tell me a fun fact about history.\")]}\n",
    "\n",
    "for event in app.stream(inputs_2):\n",
    "    for key, value in event.items():\n",
    "        print(f\"Finished running: {key}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
